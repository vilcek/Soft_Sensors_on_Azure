{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ac755f-3c46-4177-8f0c-5cb5fcefa921",
   "metadata": {},
   "source": [
    "Import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6a3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, LinkedService\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.data import OutputFileDatasetConfig, HDFSOutputDatasetConfig\n",
    "from azureml.core.compute import SynapseCompute, ComputeTarget, AmlCompute, ComputeTarget\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_GPU_IMAGE\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep, SynapseSparkStep\n",
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline, StepSequence\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be84451e-3ab3-49c7-ad92-05b9b465ab5c",
   "metadata": {},
   "source": [
    "This code was tested on the Azure ML SDK 1.36.0 version. Please make sure you have this version installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7814d80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.36.0\n"
     ]
    }
   ],
   "source": [
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97f3d2-7933-498f-ab44-85710d9a6bd5",
   "metadata": {},
   "source": [
    "Get a reference to the Azure ML Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf2aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7282b75-b58e-4317-8289-7e67995f785d",
   "metadata": {},
   "source": [
    "Here we define the name and key for the storage account we use to store the input, intermediate, and output data.\n",
    "\n",
    "To make the configuration easier, we are using the default Azure Blob Storage account created with the Azure ML Workspace. In a real-world implementation, one would probably have that data in an [Azure Data Lake Storage](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction) account instead.\n",
    "\n",
    "Please make sure you enter your own storage account name and key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586e29dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_name = '<your storage account name>'\n",
    "account_key = '<your storage account key>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d05abf-71d8-479a-a349-809d5f5713ff",
   "metadata": {},
   "source": [
    "For the data to be accessed by the pipeline steps to be defined here, we first create [Azure ML Datasets](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-with-datasets) pointing to the storage locations where the data will be read from or written to.\n",
    "\n",
    "The 'raw_dataset' object is defined as an input dataset pointing to the raw data storage location.\n",
    "\n",
    "The 'prepared_dataset' and 'featurized_dataset' objects are defined as output datasets pointing to storage locations to be written by the data preparation and feature selection steps, respectively. They are intermediate datasets and will also be used as inputs for the feature selection and model training steps, respectively.\n",
    "\n",
    "The 'featurized_dataset' object is not directly consumed as input by the model training step. Instead, it is first referenced by the 'model_input_dataset' object as a partitioned file dataset, for each partition (the partitioning is by training task and output sensor name) to be processed independently, in parallel.\n",
    "\n",
    "The 'model_output_dir' object is defined as an output dataset pointing to the storage location to be written by the model training step.\n",
    "\n",
    "The 'raw_dataset', 'prepared_dataset', and 'featurized_dataset' objects are defined as HDFS datasets because they are accessed by Apache Spark. The 'model_input_dataset' and 'model_output_dir' objects are defined as file datasets because they are accessed by standard Python running on Azure ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ca3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datastore_name = 'softsensors_raw_datastore'\n",
    "raw_container_name = 'softsensors-raw'\n",
    "raw_datastore = Datastore.register_azure_blob_container(workspace=ws, account_name=account_name, account_key=account_key, datastore_name=raw_datastore_name, container_name=raw_container_name, create_if_not_exists=True)\n",
    "raw_dataset = Dataset.File.from_files(path=(raw_datastore, '/*.parquet'), validate=False).as_named_input('raw_dataset').as_hdfs()\n",
    "\n",
    "prepared_datastore_name = 'softsensors_prepared_datastore'\n",
    "prepared_container_name = 'softsensors-prepared'\n",
    "prepared_datastore = Datastore.register_azure_blob_container(workspace=ws, account_name=account_name, account_key=account_key, datastore_name=prepared_datastore_name, container_name=prepared_container_name, create_if_not_exists=True)\n",
    "prepared_dataset = HDFSOutputDatasetConfig(name='prepared_dataset', destination=(prepared_datastore, '/data'))\n",
    "\n",
    "featurized_datastore_name = 'softsensors_featurized_datastore'\n",
    "featurized_container_name = 'softsensors-featurized'\n",
    "featurized_datastore = Datastore.register_azure_blob_container(workspace=ws, account_name=account_name, account_key=account_key, datastore_name=featurized_datastore_name, container_name=featurized_container_name, create_if_not_exists=True)\n",
    "featurized_dataset = HDFSOutputDatasetConfig(name='featurized_dataset', destination=(featurized_datastore, '/data'))\n",
    "\n",
    "model_input_dataset = Dataset.File.from_files(path=(featurized_datastore, 'data/train/*/*/*'), partition_format='data/train/{task}/{target}/*', validate=False)\n",
    "model_datastore_name = 'softsensors_models_datastore'\n",
    "model_container_name = 'softsensors-models'\n",
    "model_datastore = Datastore.register_azure_blob_container(workspace=ws, account_name=account_name, account_key=account_key, datastore_name=model_datastore_name, container_name=model_container_name, create_if_not_exists=True)\n",
    "model_output_dir = OutputFileDatasetConfig(name='model_output_dir', destination=(model_datastore, '/model_outputs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530893f-691f-4570-b256-cc0c94d29da1",
   "metadata": {},
   "source": [
    "Here we [link our Azure Synapse Spark pool with our Azure ML workspace and create an Azure ML Compute Target pointing to it](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-link-synapse-ml-workspaces). This compute target will be used to run the data preparation and feature selection steps.\n",
    "\n",
    "Please make sure you use your own definitions for the Azure Synapse linked service and Spark pool names below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8625fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class LinkedService: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SynapseCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provisioning operation finished, operation \"Succeeded\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class SynapseCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "synapse_linked_service = '<your Azure Synapse linked service name>'\n",
    "synapse_pool_name = '<your Azure Synapse Spark pool name>'\n",
    "synapse_compute_name = 'sparkcpu1'\n",
    "\n",
    "linked_service = LinkedService.get(ws, synapse_linked_service)\n",
    "\n",
    "attach_config = SynapseCompute.attach_configuration(\n",
    "    linked_service = linked_service,\n",
    "    type='SynapseSpark',\n",
    "    pool_name=synapse_pool_name)\n",
    "\n",
    "synapse_compute_target=ComputeTarget.attach(\n",
    "    workspace=ws,\n",
    "    name=synapse_compute_name,\n",
    "    attach_configuration=attach_config)\n",
    "\n",
    "synapse_compute_target.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a8424-c5d2-4daa-a659-e43c9ec40f2e",
   "metadata": {},
   "source": [
    "Define the [pipeline steps to be run on Azure Synapse](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-synapsesparkstep).\n",
    "\n",
    "As part of this definition, we can also define an execution environment and add any package dependencies to it. We also specify the name and location of the PySpark script to be run, inputs and outputs datasets, arbitrary arguments, and the compute target we created before with its resource configuration.\n",
    "\n",
    "Please make sure you use your own Azure Anomaly Detector key and service location values in the arguments definition for the 'data_preparation_step' object below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069f77b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class SynapseSparkStep: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SynapseSparkStep: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only conda_dependencies specified in environment will be used in Synapse Spark run.\n",
      "only conda_dependencies specified in environment will be used in Synapse Spark run.\n"
     ]
    }
   ],
   "source": [
    "synapse_env = Environment(name='synapse_softsensor_environment')\n",
    "synapse_env.python.conda_dependencies.add_pip_package('azureml-core>=1.20.0')\n",
    "\n",
    "data_preparation_step = SynapseSparkStep(\n",
    "    name = 'data_preparation_step',\n",
    "    file = 'data_preparation.py',\n",
    "    source_directory='./azure_synapse_code', \n",
    "    inputs=[raw_dataset],\n",
    "    outputs=[prepared_dataset],\n",
    "    arguments = ['--input_dataset',raw_dataset, '--output_dataset',prepared_dataset,\n",
    "              '--anomaly_key','<your azure anomaly detector key>', '--anomaly_service_location','<your azure anomaly detector service location>',\n",
    "              '--anomaly_max_data',8640, '--anomaly_min_data',12],\n",
    "    compute_target = 'sparkcpu1',\n",
    "    driver_memory = '16g',\n",
    "    driver_cores = 8,\n",
    "    executor_memory = '16g',\n",
    "    executor_cores = 8,\n",
    "    num_executors = 4,\n",
    "    environment = synapse_env)\n",
    "\n",
    "feature_selection_step = SynapseSparkStep(\n",
    "    name = 'data_featurization_step',\n",
    "    file = 'feature_selection.py',\n",
    "    source_directory='./azure_synapse_code', \n",
    "    inputs=[prepared_dataset.as_input().as_hdfs()],\n",
    "    outputs=[featurized_dataset],\n",
    "    arguments = ['--input_dataset',prepared_dataset.as_input().as_hdfs(), '--output_dataset',featurized_dataset, '--start_test_date','1970-01-10 08:00:00'],\n",
    "    compute_target = 'sparkcpu1',\n",
    "    driver_memory = '16g',\n",
    "    driver_cores = 8,\n",
    "    executor_memory = '16g',\n",
    "    executor_cores = 8,\n",
    "    num_executors = 4,\n",
    "    environment = synapse_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73503d3e-c0b3-47fb-91bb-b7f420fa745c",
   "metadata": {},
   "source": [
    "Here we create the [Azure ML Compute Target](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target) to be used for running the model training step. As we are going to train a deep learning-based model, it is advised to use at least GPUs of type Standard NC6. We define the maximum number of nodes in the compute target cluster as 2, so that we can run the model training in parallel for each of the 2 soft sensors to be modeled in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7566239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class SynapseCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SynapseCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. gpucluster\n"
     ]
    }
   ],
   "source": [
    "aml_compute_name = 'gpucluster'\n",
    "aml_compute_min_nodes = 0\n",
    "aml_compute_max_nodes = 2\n",
    "aml_vm_size = 'STANDARD_NC6'\n",
    "\n",
    "if aml_compute_name in ws.compute_targets:\n",
    "    aml_compute_target = ws.compute_targets[aml_compute_name]\n",
    "    if aml_compute_target and type(aml_compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + aml_compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = aml_vm_size,\n",
    "                                                                min_nodes = aml_compute_min_nodes, \n",
    "                                                                max_nodes = aml_compute_max_nodes)\n",
    "\n",
    "    aml_compute_target = ComputeTarget.create(ws, aml_compute_name, provisioning_config)\n",
    "    aml_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    print(aml_compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46996e2a-125d-42b4-b1c5-dc57593040d1",
   "metadata": {},
   "source": [
    "Define the [pipeline step to be run on Azure ML as a parallel run step](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/machine-learning-pipelines/parallel-run).\n",
    "\n",
    "As part of this definition, we can also define an execution environment and add any package dependencies to it. We also specify a configuration object with the name and location of the Python script to be run, data partitioning schema and execution control parameters. In the parallel run step definition we also have inputs and outputs datasets and arbitrary arguments for the Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c48d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_conda_deps = CondaDependencies.create(pip_packages=['azureml-core', 'azureml-dataset-runtime[fuse]', 'pandas', 'ipython', 'tsai'])\n",
    "aml_env = Environment(name='aml_softsensor_environment')\n",
    "aml_env.python.conda_dependencies = aml_conda_deps\n",
    "aml_env.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "\n",
    "model_training_config = ParallelRunConfig(\n",
    "    source_directory='./azure_ml_code',\n",
    "    entry_script='model_training.py',\n",
    "    partition_keys=model_input_dataset.partition_keys,\n",
    "    error_threshold=5,\n",
    "    output_action='append_row',\n",
    "    append_row_file_name='soft_sensor_predictions.txt',\n",
    "    environment=aml_env,\n",
    "    compute_target=aml_compute_target, \n",
    "    node_count=2,\n",
    "    run_invocation_timeout=10000\n",
    ")\n",
    "\n",
    "model_training_step = ParallelRunStep(\n",
    "    name='model_training_step',\n",
    "    inputs=[model_input_dataset.as_named_input('model_input_dataset')],\n",
    "    output=model_output_dir,\n",
    "    arguments=['--output_dir',model_output_dir,\n",
    "               '--regression_window_length',9, '--regression_stride',1, '--regression_horizon',0,\n",
    "               '--batch_size',512, '--max_epochs',500,\n",
    "               '--arch','GRU',\n",
    "               '--hidden_size',128, '--n_layers',1, '--bias','True', '--rnn_dropout',0.2, '--bidirectional','True', '--fc_dropout',0.1,\n",
    "               '--min_delta',0.001, '--patience',15],\n",
    "    parallel_run_config=model_training_config,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b3b7c-123e-4d92-a34c-10a36cc4040b",
   "metadata": {},
   "source": [
    "After defining the pipeline steps, we pack them into a StepSequence object to establish an execution sequence dependency. We then use it to create a Pipeline object and submit it for execution through an [Azure ML Experiment](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#experiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e6c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class SynapseCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step data_preparation_step [6513b8d4][acd3e051-b064-46cf-a448-597e5d75b266], (This step will run and generate new outputs)\n",
      "Created step data_featurization_step [79a0a7e2][b7d2227f-c72a-44fa-9115-7338c32442a5], (This step will run and generate new outputs)\n",
      "Created step model_training_step [2c6f64e1][4b1fc141-7692-480e-a94b-49a77af72d87], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 4a2dba18-9e64-42a4-8ad8-22b11b0b83bc\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/4a2dba18-9e64-42a4-8ad8-22b11b0b83bc?wsid=/subscriptions/a6c2a7cc-d67e-4a1a-b765-983f08c0423a/resourcegroups/alvilcek-ml-rg/workspaces/alvilcek-ml-workspace&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "source": [
    "step_sequence = StepSequence(steps=[data_preparation_step, feature_selection_step, model_training_step])\n",
    "pipeline = Pipeline(workspace=ws, steps=step_sequence)\n",
    "\n",
    "pipeline_run = Experiment(ws, 'softsensor_model_training').submit(pipeline, regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff31b9-15a7-469a-b361-29b1c21cd765",
   "metadata": {},
   "source": [
    "The experiment submission above is asynchronous, so you can call wait_for_completion on the Experiment object to get run notifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85872504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 4a2dba18-9e64-42a4-8ad8-22b11b0b83bc\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/4a2dba18-9e64-42a4-8ad8-22b11b0b83bc?wsid=/subscriptions/a6c2a7cc-d67e-4a1a-b765-983f08c0423a/resourcegroups/alvilcek-ml-rg/workspaces/alvilcek-ml-workspace&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '4a2dba18-9e64-42a4-8ad8-22b11b0b83bc', 'status': 'Completed', 'startTimeUtc': '2022-04-12T23:29:05.011173Z', 'endTimeUtc': '2022-04-12T23:47:40.419253Z', 'services': {}, 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.pipelineComponent': 'pipelinerun'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://alvilcekmlwork0917427776.blob.core.windows.net/azureml/ExperimentRun/dcid.4a2dba18-9e64-42a4-8ad8-22b11b0b83bc/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=HXFCEnMl5%2BB%2BwTleePgrvMIw8uN38tFeY0nCE%2F1mspE%3D&skoid=6d8efc04-c3a3-4cb2-ab5c-74ac9eddbbac&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-04-12T16%3A32%3A32Z&ske=2022-04-14T00%3A42%3A32Z&sks=b&skv=2019-07-07&st=2022-04-12T23%3A37%3A47Z&se=2022-04-13T07%3A47%3A47Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://alvilcekmlwork0917427776.blob.core.windows.net/azureml/ExperimentRun/dcid.4a2dba18-9e64-42a4-8ad8-22b11b0b83bc/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=gMh5bnBnb1ezBJRz%2B0T5KtnKIYYz%2BNSiuGh06MgYp9g%3D&skoid=6d8efc04-c3a3-4cb2-ab5c-74ac9eddbbac&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-04-12T16%3A32%3A32Z&ske=2022-04-14T00%3A42%3A32Z&sks=b&skv=2019-07-07&st=2022-04-12T23%3A37%3A47Z&se=2022-04-13T07%3A47%3A47Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://alvilcekmlwork0917427776.blob.core.windows.net/azureml/ExperimentRun/dcid.4a2dba18-9e64-42a4-8ad8-22b11b0b83bc/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=ZICFzBkdJROf4MO11fB3F1RbZJ%2F8zsH7JvdkOYDR%2FA4%3D&skoid=6d8efc04-c3a3-4cb2-ab5c-74ac9eddbbac&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-04-12T16%3A32%3A32Z&ske=2022-04-14T00%3A42%3A32Z&sks=b&skv=2019-07-07&st=2022-04-12T23%3A37%3A47Z&se=2022-04-13T07%3A47%3A47Z&sp=r'}, 'submittedBy': 'Alexandre Vilcek'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
